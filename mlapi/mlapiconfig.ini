[general]
# This is an optional file
# If specified, you can specify tokens with secret values in that file
# and onlt refer to the tokens in your main config file

#secrets=./secrets.ini
secrets=$MLAPI_DIR/secrets.ini

# portal/user/password are needed if you plan on using ZM's 
# auth mechanism to get images
# portal=!ZM_PORTAL
# user=!ZM_USER
# password=!ZM_PASSWORD
#basic_auth_user=username
#basic_auth_password=password

# api portal is needed if you plan to use tokens to get images
# requires ZM 1.33 or above
# api_portal=!ZM_API_PORTAL

# make this no, if you don't plan to use auth. Default is yes.
auth_enabled=yes

# port that mlapi will listen on. Default 5000
port=5000

# Maximum # of processes that will be forked
# to handle requests. Note that each process will
# have its own copy of the model, so memory can 
# build up very quickly
# This number also dictates how many requests will be executed in parallel
# The rest will be queued

# default: flask
wsgi_server=bjoern 

# if yes, will use ZM logs. Default no
#use_zm_logs=no
use_zm_logs=no
pyzm_overrides={'log_level_debug':5}

# If you are using bjoern, processes is always 1 
# For now, keep this to 1 if you are on a GPU
processes=1

# the secret key that will be used to sign
# JWT tokens. Make sure you change the value
# in your secrets.ini
mlapi_secret_key=!MLAPI_SECRET_KEY

# base data path for various files the ES+OD needs
# we support in config variable substitution as well
base_data_path=/var/lib/zmeventnotification
#base_data_path=.
# folder where images will be uploaded
# default ./images
images_path={{base_data_path}}/images

# folder where the user DB will be stored
db_path=$MLAPI_DIR/db


# If yes, will allow connections to self signed certificates
# Default yes
allow_self_signed=yes


# You can now limit the # of detection process
# per target processor. If not specified, default is 1
# Other detection processes will wait to acquire lock

cpu_max_processes=3
tpu_max_processes=1
gpu_max_processes=1

# NEW: Time to wait in seconds per processor to be free, before
# erroring out. Default is 120 (2 mins)
cpu_max_lock_wait=120
tpu_max_lock_wait=120
gpu_max_lock_wait=120

model_sequence=object,face,alpr


# If yes, will import zm zones defined for monitors. Default is no
#import_zm_zones=yes

# If enabled, will only filter zone names that match the alarm cause 
# This is useful if you only want to report detections where motion 
# was detected by ZM. Default no
#only_triggered_zm_zones=no

# if yes, last detection will be stored for monitors
# and bounding boxes that match, along with labels
# will be discarded for new detections. This may be helpful
# in getting rid of static objects that get detected
# due to some motion. 
match_past_detections=no

# The max difference in area between the objects if match_past_detection is on
# can also be specified in px like 300px. Default is 5%. Basically, bounding boxes of the same
# object can slightly differ ever so slightly between detection. Contributor @neillbell put in this PR
# to calculate the difference in areas and based on his tests, 5% worked well. YMMV. Change it if needed.
# Note: You can specify label/object specific max_diff_areas as well. If present, they override this value
# example: 
# person_past_det_max_diff_area=5%
# car_past_det_max_diff_area=5000px
past_det_max_diff_area=5%

# this is the maximum size a detected object can have. You can specify it in px or % just like past_det_max_diff_area 
# This is pretty useful to eliminate bogus detection. In my case, depending on shadows and other lighting conditions, 
# I sometimes see "car" or "person" detected that covers most of my driveway view. That is practically impossible 
# and therefore I set mine to 70% because I know any valid detected objected cannot be larger than that area

max_detection_size=90%

# config for object
[object]

# If you are using legacy format (use_sequence=no) then these parameters will 
# be used during ML inferencing
#object_detection_pattern=.*
object_detection_pattern=(person|car|motorbike|bus|truck|boat)
object_min_confidence=0.3
object_framework=coral_edgetpu
object_processor=tpu
object_weights={{base_data_path}}/models/coral_edgetpu/ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite
object_labels={{base_data_path}}/models/coral_edgetpu/coco_indexed.names

# If you are using the new ml_sequence format (use_sequence=yes) then 
# you can fiddle with these parameters and look at ml_sequence later
# Note that these can be named anything. You can add custom variables, ad-infinitum


# This is a useful debugging trick. If you are chaning models and want to know which
# model detected an object, make this yes. When yes, it will prefix the model name before the
# detected object. Example: Instead of 'person', it will say '(yolo) person'
show_models=no

# Google Coral
# The mobiledet model came out in Nov 2020 and is supposed to be faster and more accurate but YMMV
tpu_object_weights_mobiledet={{base_data_path}}/models/coral_edgetpu/ssdlite_mobiledet_coco_qat_postprocess_edgetpu.tflite
tpu_object_weights_mobilenet={{base_data_path}}/models/coral_edgetpu/ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite
tpu_object_weights_yolov5={{base_data_path}}/models/coral_edgetpu/yolov5s-int8_edgetpu.tflite
tpu_object_labels={{base_data_path}}/models/coral_edgetpu/coco_indexed.names
tpu_object_framework=coral_edgetpu
tpu_object_processor=tpu
tpu_min_confidence=0.6


# Yolo v4 on GPU (falls back to CPU if no GPU)
yolo4_object_weights={{base_data_path}}/models/yolov4/yolov4.weights
yolo4_object_labels={{base_data_path}}/models/yolov4/coco.names
yolo4_object_config={{base_data_path}}/models/yolov4/yolov4.cfg
yolo4_object_framework=opencv
yolo4_object_processor=gpu

# Yolo v3 on GPU (falls back to CPU if no GPU)
yolo3_object_weights={{base_data_path}}/models/yolov3/yolov3.weights
yolo3_object_labels={{base_data_path}}/models/yolov3/coco.names
yolo3_object_config={{base_data_path}}/models/yolov3/yolov3.cfg
yolo3_object_framework=opencv
yolo3_object_processor=gpu

# Tiny Yolo V4 on GPU (falls back to CPU if no GPU)
tinyyolo_object_config={{base_data_path}}/models/tinyyolov4/yolov4-tiny.cfg
tinyyolo_object_weights={{base_data_path}}/models/tinyyolov4/yolov4-tiny.weights
tinyyolo_object_labels={{base_data_path}}/models/tinyyolov4/coco.names
tinyyolo_object_framework=opencv
tinyyolo_object_processor=gpu



[face]

# NOTE: None of these are used if use_sequence is enabled. Ig enabled
# only values in ml_sequence are processed


face_detection_framework=dlib
face_recognition_framework=dlib
face_num_jitters=0
face_upsample_times=1
face_model=cnn
face_train_model=cnn
face_recog_dist_threshold=0.6
face_recog_knn_algo=ball_tree
known_images_path={{base_data_path}}/known_faces
unknown_images_path={{base_data_path}}/unknown_faces

unknown_face_name=unknown face
save_unknown_faces=yes
save_unknown_faces_leeway_pixels=50

[alpr]

# NOTE: None of these are used if use_sequence is enabled. Ig enabled
# only values in ml_sequence are processed


alpr_use_after_detection_only=yes
alpr_api_type=cloud

# -----| If you are using plate recognizer | ------
alpr_service=plate_recognizer
alpr_key=!PLATEREC_ALPR_KEY
platerec_stats=yes
#platerec_regions=['us','cn','kr']
platerec_min_dscore=0.1
platerec_min_score=0.2

# ----| If you are using openALPR |-----
#alpr_service=open_alpr
#alpr_key=!OPENALPR_ALPR_KEY
#openalpr_recognize_vehicle=1
#openalpr_country=us
#openalpr_state=ca
# openalpr returns percents, but we convert to between 0 and 1
#openalpr_min_confidence=0.3

# ----| If you are using openALPR command line |-----
openalpr_cmdline_binary=alpr
openalpr_cmdline_params=-j -d
openalpr_cmdline_min_confidence=0.3


## Monitor specific settings
# You can override any parameter on a per monitor basis
# The format is [monitor-N] where N is the monitor id

[monitor-9998]
# doorbell
model_sequence=face
object_detection_pattern=(person|monitor_doorbell)
valid_face_area=184,235 1475,307 1523,1940 146,1940
match_past_detections=yes


[monitor-9999]
#deck
object_detection_pattern=(person|monitor_deck)
stream_sequence = {
        'frame_strategy': 'most_models',
        'frame_set': 'alarm',
        'contig_frames_before_error': 5,
        'max_attempts': 3,
        'sleep_between_attempts': 4,
        'resize':800

    }

[ml]
# if enabled, will not grab exclusive locks before running inferencing
# locking seems to cause issues on some unique file systems
disable_locks = no
my_frame_strategy = most_models

use_sequence = yes

stream_sequence = {
		'frame_strategy': '{{my_frame_strategy}}',
		'frame_set': 'snapshot,alarm',
		'contig_frames_before_error': 5,
		'max_attempts': 3,
		'sleep_between_attempts': 4,
		'resize':800,
		# if yes, will convert 'snapshot' to a specific frame id
		# This is useful because you may see boxes drawn at the wrong places when using mlapi 
		# This is because when mlapi detects an image, a 'snapshot' could point to, say, frame 45
		# But when zm_detect gets the detections back and draws the boxes, snapshot could have moved 
		# to frame 50 (example). Enabling this makes sure mlapi tells zm_detect which frame id to use
		# default is 'no'
		'convert_snapshot_to_fid': 'yes', 

	} # very important - this brace needs to be indented inside stream_sequence

ml_sequence= {
		'general': {
			'model_sequence': '{{model_sequence}}',
			'disable_locks': '{{disable_locks}}',
			'match_past_detections': '{{match_past_detections}}',
			'past_det_max_diff_area': '5%',
			'car_past_det_max_diff_area': '10%'

		},
		'object': {
			'general':{
				'pattern':'{{object_detection_pattern}}',
				'same_model_sequence_strategy': 'most_unique' # also 'most', 'most_unique's
			},
			'sequence': [{
				#First run on TPU with higher confidence
				#'maxsize':320,
				'name': 'TPU object detection',
				'enabled': 'no',
				'object_weights':'{{tpu_object_weights_mobiledet}}',
				'object_labels': '{{tpu_object_labels}}',
				'object_min_confidence': {{tpu_min_confidence}},
				'object_framework':'{{tpu_object_framework}}',
				'tpu_max_processes': {{tpu_max_processes}},
				'tpu_max_lock_wait': {{tpu_max_lock_wait}},
				'max_detection_size':'{{max_detection_size}}',
				'show_models':'{{show_models}}',

			},
			{
				# YoloV4 on GPU if TPU fails (because sequence strategy is 'first')
				'name': 'CPU/GPU Yolov4 Object Detection',
				'enabled': 'yes',
				'object_config':'{{yolo4_object_config}}',
				'object_weights':'{{yolo4_object_weights}}',
				'object_labels': '{{yolo4_object_labels}}',
				'object_min_confidence': {{object_min_confidence}},
				'object_framework':'{{yolo4_object_framework}}',
				'object_processor': '{{yolo4_object_processor}}',
				'gpu_max_processes': {{gpu_max_processes}},
				'gpu_max_lock_wait': {{gpu_max_lock_wait}},
				'cpu_max_processes': {{cpu_max_processes}},
				'cpu_max_lock_wait': {{cpu_max_lock_wait}},
				'max_detection_size':'{{max_detection_size}}',
				'match_past_detections': 'yes',
				'past_det_max_diff_area': '5%',
				'show_models':'{{show_models}}'

			}]
		},
		'face': {
			'general':{
				'pattern': '{{face_detection_pattern}}',
				'same_model_sequence_strategy': 'union' # combine results below
			},
			'sequence': [
			{
				'name': 'Face Detection (TPU)',
				'enabled': 'no', # make this yes if you want face detection with TPU first
				'face_detection_framework': 'tpu',
				'face_weights':'/var/lib/zmeventnotification/models/coral_edgetpu/ssd_mobilenet_v2_face_quant_postprocess_edgetpu.tflite',
				'face_min_confidence': 0.3 
			},
			{
				'name':'Face Recognition (Dlib)', # optional
				'enabled': 'yes', # optional
				# 'pre_existing_labels': ['face'], # If you use TPU detection first, we can run this ONLY if TPU detects a face first
				'save_unknown_faces':'{{save_unknown_faces}}',
				'save_unknown_faces_leeway_pixels':{{save_unknown_faces_leeway_pixels}},
				'face_detection_framework': '{{face_detection_framework}}',
				'known_images_path': '{{known_images_path}}',
				'unknown_images_path': '{{unknown_images_path}}',
				'face_model': '{{face_model}}',
				'face_train_model': '{{face_train_model}}',
				'face_recog_dist_threshold': {{face_recog_dist_threshold}},
				'face_num_jitters': {{face_num_jitters}},
				'face_upsample_times':{{face_upsample_times}},
				'gpu_max_processes': {{gpu_max_processes}},
				'gpu_max_lock_wait': {{gpu_max_lock_wait}},
				'cpu_max_processes': {{cpu_max_processes}},
				'cpu_max_lock_wait': {{cpu_max_lock_wait}},
				'max_size':800
			}]
		},

		'alpr': {
			'general':{
				'same_model_sequence_strategy': 'first',
				'pre_existing_labels':['car', 'motorbike', 'bus', 'truck', 'boat'],
				'pattern': '{{alpr_detection_pattern}}'

			},
			'sequence': [{
				'name': 'Platerecognizer Cloud Service',
				'enabled': 'yes',
				'alpr_api_type': '{{alpr_api_type}}',
				'alpr_service': '{{alpr_service}}',
				'alpr_key': '{{alpr_key}}',
				'platrec_stats': '{{platerec_stats}}',
				'platerec_min_dscore': {{platerec_min_dscore}},
				'platerec_min_score': {{platerec_min_score}},
				'max_size':1600,
				#'platerec_payload': {
				#'regions':['us'],
				#'camera_id':12,
				#},
				#'platerec_config': {
				#    'region':'strict',
				#    'mode': 'fast'
				#}
			}]
		}
	} # very important - this brace needs to be indented inside ml_sequence

